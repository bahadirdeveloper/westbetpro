"""
CONFIDENCE SKORLAMA SÄ°STEMÄ°
GeÃ§miÅŸ verileri kullanarak kurallarÄ±n baÅŸarÄ± oranÄ±nÄ± hesaplar
"""

import os
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from datetime import datetime

from golden_rules import GOLDEN_RULES, GoldenRule, get_rule_by_id


class OpportunityScorer:
    """
    GeÃ§miÅŸ verilere dayalÄ± confidence skorlama sistemi

    AkÄ±ÅŸ:
    1. history_candidates.pkl verilerini oku
    2. Her kural iÃ§in baÅŸarÄ± oranÄ±nÄ± hesapla
    3. Confidence skorlarÄ±nÄ± dinamik ayarla
    4. Backtest raporunu Ã¼ret
    """

    def __init__(self,
                 history_pkl_path: str = "data/history_candidates.pkl",
                 output_report_path: str = "data/scorer_report.json"):
        """
        Args:
            history_pkl_path: GeÃ§miÅŸ veriler pickle dosyasÄ±
            output_report_path: Ã‡Ä±ktÄ± rapor dosyasÄ±
        """
        self.history_pkl_path = history_pkl_path
        self.output_report_path = output_report_path
        self.golden_rules = GOLDEN_RULES

    def load_history_data(self) -> pd.DataFrame:
        """GeÃ§miÅŸ verileri yÃ¼kle"""
        if not os.path.exists(self.history_pkl_path):
            print(f"âš ï¸  UYARI: {self.history_pkl_path} bulunamadÄ±!")
            return pd.DataFrame()

        df = pd.read_pickle(self.history_pkl_path)
        print(f"âœ… GeÃ§miÅŸ veri yÃ¼klendi: {len(df):,} satÄ±r")
        return df

    def parse_prediction_result(self, prediction: str, row: pd.Series) -> Optional[bool]:
        """
        Tahminin doÄŸru Ã§Ä±kÄ±p Ã§Ä±kmadÄ±ÄŸÄ±nÄ± kontrol et

        Args:
            prediction: Tahmin string'i (Ã¶rn: "MS 1.5 ÃœST")
            row: MaÃ§ satÄ±rÄ± (SKOR MS, SKOR Ä°Y iÃ§ermeli)

        Returns:
            True = doÄŸru, False = yanlÄ±ÅŸ, None = kontrol edilemedi
        """
        skor_ms = row.get("SKOR MS")
        skor_iy = row.get("SKOR Ä°Y")

        # Skor yoksa kontrol edilemez
        if pd.isna(skor_ms):
            return None

        try:
            # Skoru parse et (Ã¶rn: "2-1" -> (2, 1))
            if isinstance(skor_ms, str) and "-" in skor_ms:
                ev, dep = map(int, skor_ms.split("-"))
            else:
                return None

            total_goals_ms = ev + dep

            # Ä°Y skoru varsa parse et
            if pd.notna(skor_iy) and isinstance(skor_iy, str) and "-" in skor_iy:
                iy_ev, iy_dep = map(int, skor_iy.split("-"))
                total_goals_iy = iy_ev + iy_dep
            else:
                total_goals_iy = None

        except (ValueError, AttributeError):
            return None

        # Tahmin tipine gÃ¶re doÄŸruluk kontrolÃ¼
        pred_upper = prediction.upper()

        # â”€â”€ MS (MaÃ§ Sonucu) Kontrolleri â”€â”€
        if "MS" in pred_upper and "Ä°Y" not in pred_upper:
            if "0.5 ÃœST" in pred_upper:
                return total_goals_ms >= 1
            elif "1.5 ÃœST" in pred_upper:
                return total_goals_ms >= 2
            elif "2.5 ÃœST" in pred_upper:
                return total_goals_ms >= 3
            elif "3.5 ÃœST" in pred_upper:
                return total_goals_ms >= 4
            elif "2.5 ALT" in pred_upper:
                return total_goals_ms < 3
            elif "3.5 ALT" in pred_upper:
                return total_goals_ms < 4

        # â”€â”€ Ä°Y (Ä°lk YarÄ±) Kontrolleri â”€â”€
        elif "Ä°Y" in pred_upper and total_goals_iy is not None:
            if "0.5 ÃœST" in pred_upper:
                return total_goals_iy >= 1
            elif "1.5 ÃœST" in pred_upper:
                return total_goals_iy >= 2
            elif "0.5 ALT" in pred_upper:
                return total_goals_iy < 1
            elif "1.5 ALT" in pred_upper:
                return total_goals_iy < 2

        # â”€â”€ KG VAR (KarÅŸÄ±lÄ±klÄ± Gol) â”€â”€
        elif "KG VAR" in pred_upper or "T.GOL" in pred_upper:
            if total_goals_iy is not None:
                # Ä°Y iÃ§in
                if "Ä°Y" in pred_upper:
                    return iy_ev > 0 and iy_dep > 0
            # MS iÃ§in
            return ev > 0 and dep > 0

        # â”€â”€ EV/DEP Gol Kontrolleri â”€â”€
        elif "EV" in pred_upper and "MS" in pred_upper:
            if "0.5 ÃœST" in pred_upper:
                return ev >= 1
            elif "1.5 ÃœST" in pred_upper:
                return ev >= 2

        elif "DEP" in pred_upper and "MS" in pred_upper:
            if "0.5 ÃœST" in pred_upper:
                return dep >= 1
            elif "1.5 ÃœST" in pred_upper:
                return dep >= 2

        # TanÄ±nmayan tahmin tipi
        return None

    def calculate_rule_accuracy(self, rule_id: int, history_df: pd.DataFrame) -> Dict:
        """
        Bir kuralÄ±n baÅŸarÄ± oranÄ±nÄ± hesapla

        Args:
            rule_id: Kural ID
            history_df: GeÃ§miÅŸ veriler

        Returns:
            {
                "rule_id": int,
                "total_matches": int,
                "predictions_tested": int,
                "successful": int,
                "failed": int,
                "accuracy": float,
                "confidence_adjustment": int
            }
        """
        rule = get_rule_by_id(rule_id)
        if not rule:
            return {}

        # Bu kurala ait geÃ§miÅŸ maÃ§lar
        rule_matches = history_df[history_df["senaryo_id"] == rule_id]

        if len(rule_matches) == 0:
            return {
                "rule_id": rule_id,
                "rule_name": rule.name,
                "total_matches": 0,
                "predictions_tested": 0,
                "successful": 0,
                "failed": 0,
                "accuracy": 0.0,
                "confidence_adjustment": 0
            }

        # Her tahmin iÃ§in sonuÃ§larÄ± topla
        prediction_results = defaultdict(lambda: {"success": 0, "fail": 0})

        for _, row in rule_matches.iterrows():
            for prediction in rule.predictions:
                result = self.parse_prediction_result(prediction, row)
                if result is not None:
                    if result:
                        prediction_results[prediction]["success"] += 1
                    else:
                        prediction_results[prediction]["fail"] += 1

        # Toplam baÅŸarÄ±/baÅŸarÄ±sÄ±zlÄ±k
        total_success = sum(p["success"] for p in prediction_results.values())
        total_fail = sum(p["fail"] for p in prediction_results.values())
        total_tested = total_success + total_fail

        # Accuracy hesapla
        accuracy = (total_success / total_tested * 100) if total_tested > 0 else 0.0

        # Confidence adjustment hesapla
        # 90%+ accuracy: +5
        # 85-90%: +2
        # 80-85%: 0
        # 75-80%: -2
        # <75%: -5
        if accuracy >= 90:
            adjustment = 5
        elif accuracy >= 85:
            adjustment = 2
        elif accuracy >= 80:
            adjustment = 0
        elif accuracy >= 75:
            adjustment = -2
        else:
            adjustment = -5

        return {
            "rule_id": rule_id,
            "rule_name": rule.name,
            "total_matches": len(rule_matches),
            "predictions_tested": total_tested,
            "successful": total_success,
            "failed": total_fail,
            "accuracy": round(accuracy, 2),
            "confidence_adjustment": adjustment,
            "prediction_breakdown": dict(prediction_results)
        }

    def generate_backtest_report(self, history_df: pd.DataFrame) -> Dict:
        """
        TÃ¼m kurallar iÃ§in backtest raporu Ã¼ret

        Args:
            history_df: GeÃ§miÅŸ veriler

        Returns:
            Backtest raporu
        """
        print("ğŸ“Š Backtest raporu oluÅŸturuluyor...")

        rule_stats = []
        for rule in self.golden_rules:
            stats = self.calculate_rule_accuracy(rule.id, history_df)
            if stats:
                rule_stats.append(stats)

        # Genel istatistikler
        total_tested = sum(s["predictions_tested"] for s in rule_stats)
        total_success = sum(s["successful"] for s in rule_stats)
        total_fail = sum(s["failed"] for s in rule_stats)
        overall_accuracy = (total_success / (total_success + total_fail) * 100) if (total_success + total_fail) > 0 else 0

        # En iyi 10 kural
        top_rules = sorted(rule_stats, key=lambda x: x["accuracy"], reverse=True)[:10]

        # En kÃ¶tÃ¼ 10 kural
        worst_rules = sorted(rule_stats, key=lambda x: x["accuracy"])[:10]

        report = {
            "generated_at": datetime.now().isoformat(),
            "total_history_matches": len(history_df),
            "total_rules_tested": len(rule_stats),
            "overall_stats": {
                "total_predictions_tested": total_tested,
                "total_successful": total_success,
                "total_failed": total_fail,
                "overall_accuracy": round(overall_accuracy, 2)
            },
            "top_10_rules": top_rules,
            "worst_10_rules": worst_rules,
            "all_rule_stats": rule_stats
        }

        return report

    def save_report(self, report: Dict) -> None:
        """
        Raporu JSON formatÄ±nda kaydet

        Args:
            report: Backtest raporu
        """
        with open(self.output_report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Backtest raporu kaydedildi: {self.output_report_path}")

    def get_adjusted_confidence(self, rule_id: int, prediction: str, base_confidence: int) -> int:
        """
        Backtest sonuÃ§larÄ±na gÃ¶re adjust edilmiÅŸ confidence dÃ¶ndÃ¼r

        Args:
            rule_id: Kural ID
            prediction: Tahmin
            base_confidence: Temel confidence

        Returns:
            Adjust edilmiÅŸ confidence
        """
        # Rapor dosyasÄ± varsa oku
        if not os.path.exists(self.output_report_path):
            return base_confidence

        with open(self.output_report_path, "r", encoding="utf-8") as f:
            report = json.load(f)

        # Rule stats bul
        rule_stats = next((r for r in report["all_rule_stats"] if r["rule_id"] == rule_id), None)

        if not rule_stats:
            return base_confidence

        # Adjustment uygula
        adjusted = base_confidence + rule_stats["confidence_adjustment"]

        # 0-100 arasÄ± tut
        return max(0, min(100, adjusted))

    def run(self) -> None:
        """
        Full pipeline Ã§alÄ±ÅŸtÄ±r: Load â†’ Analyze â†’ Report
        """
        print("="*60)
        print("ğŸ“Š CONFIDENCE SKORLAMA SÄ°STEMÄ°")
        print("="*60)

        # Load history
        history_df = self.load_history_data()

        if history_df.empty:
            print("âŒ GeÃ§miÅŸ veri bulunamadÄ±!")
            return

        # Generate report
        report = self.generate_backtest_report(history_df)

        # Save
        self.save_report(report)

        # Print summary
        print("\nğŸ“ˆ Ã–ZET:")
        print(f"  Toplam Test: {report['overall_stats']['total_predictions_tested']}")
        print(f"  BaÅŸarÄ±lÄ±: {report['overall_stats']['total_successful']}")
        print(f"  BaÅŸarÄ±sÄ±z: {report['overall_stats']['total_failed']}")
        print(f"  Genel DoÄŸruluk: {report['overall_stats']['overall_accuracy']:.2f}%")

        print("\nâ­ EN Ä°YÄ° 5 KURAL:")
        for i, rule in enumerate(report["top_10_rules"][:5], 1):
            print(f"  {i}. {rule['rule_name']}")
            print(f"     DoÄŸruluk: {rule['accuracy']}% ({rule['successful']}/{rule['predictions_tested']})")

        print("\nâš ï¸  EN KÃ–TÃœ 5 KURAL:")
        for i, rule in enumerate(report["worst_10_rules"][:5], 1):
            print(f"  {i}. {rule['rule_name']}")
            print(f"     DoÄŸruluk: {rule['accuracy']}% ({rule['successful']}/{rule['predictions_tested']})")

        print("="*60)
        print("âœ… Ä°ÅŸlem tamamlandÄ±!")
        print("="*60)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI KULLANIMI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """CLI entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Confidence skorlama sistemi")
    parser.add_argument("--history", default="data/history_candidates.pkl", help="GeÃ§miÅŸ veriler pickle")
    parser.add_argument("--output", default="data/scorer_report.json", help="Ã‡Ä±ktÄ± rapor dosyasÄ±")

    args = parser.parse_args()

    # Scorer oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r
    scorer = OpportunityScorer(
        history_pkl_path=args.history,
        output_report_path=args.output
    )

    scorer.run()


if __name__ == "__main__":
    main()
